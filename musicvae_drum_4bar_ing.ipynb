{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQgDVlO7W0Xn+axGqJeVVu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUnodIFrCiJK","executionInfo":{"status":"ok","timestamp":1670373586624,"user_tz":-540,"elapsed":19804,"user":{"displayName":"강가람","userId":"05000797890779455895"}},"outputId":"bf10d744-1e99-4f5e-d3a5-920a04487080"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"h4ztA_QNHeJF","executionInfo":{"status":"ok","timestamp":1670373664974,"user_tz":-540,"elapsed":72925,"user":{"displayName":"강가람","userId":"05000797890779455895"}},"outputId":"0584536f-b1e9-4984-ab76-5cc85a964070"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting magenta==2.1.0\n","  Downloading magenta-2.1.0-py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 30.8 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (0.38.4)\n","Collecting tensor2tensor\n","  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 31.9 MB/s \n","\u001b[?25hCollecting tf-slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[K     |████████████████████████████████| 352 kB 56.0 MB/s \n","\u001b[?25hCollecting sox>=1.3.7\n","  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n","Collecting pygtrie>=2.3\n","  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (0.17.0)\n","Collecting pretty-midi>=0.2.6\n","  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 30.7 MB/s \n","\u001b[?25hCollecting dm-sonnet\n","  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n","\u001b[K     |████████████████████████████████| 254 kB 44.0 MB/s \n","\u001b[?25hCollecting apache-beam[gcp]>=2.14.0\n","  Downloading apache_beam-2.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n","\u001b[K     |████████████████████████████████| 14.5 MB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.7.3)\n","Collecting mido==1.2.6\n","  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\n","\u001b[K     |████████████████████████████████| 69 kB 8.2 MB/s \n","\u001b[?25hCollecting mir-eval>=0.4\n","  Downloading mir_eval-0.7.tar.gz (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 10.2 MB/s \n","\u001b[?25hCollecting python-rtmidi<1.2,>=1.1\n","  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\n","\u001b[K     |████████████████████████████████| 204 kB 49.5 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (3.2.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (2.9.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.15.0)\n","Collecting sk-video\n","  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 53.6 MB/s \n","\u001b[?25hCollecting note-seq\n","  Downloading note_seq-0.0.5-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 68.8 MB/s \n","\u001b[?25hCollecting numba<0.50\n","  Downloading numba-0.49.1-cp38-cp38-manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 52.5 MB/s \n","\u001b[?25hRequirement already satisfied: librosa>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (0.8.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (2.9.2)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (4.6.0)\n","Requirement already satisfied: Pillow>=3.4.2 in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (7.1.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from magenta==2.1.0) (1.21.6)\n","Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.1.1)\n","Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.8.2)\n","Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.7)\n","Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.3.0)\n","Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (9.0.0)\n","Collecting objsize<0.6.0,>=0.5.2\n","  Downloading objsize-0.5.2-py3-none-any.whl (8.2 kB)\n","Collecting cloudpickle~=2.2.0\n","  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: protobuf<4,>3.12.2 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (3.19.6)\n","Collecting pymongo<4.0.0,>=3.8.0\n","  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n","\u001b[K     |████████████████████████████████| 526 kB 62.5 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.50.0)\n","Collecting fasteners<1.0,>=0.3\n","  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n","Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.6)\n","Collecting zstandard<1,>=0.18.0\n","  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 46.5 MB/s \n","\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 64.6 MB/s \n","\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","Collecting fastavro<2,>=0.23.6\n","  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[K     |████████████████████████████████| 2.7 MB 44.5 MB/s \n","\u001b[?25hCollecting orjson<4.0\n","  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n","\u001b[K     |████████████████████████████████| 278 kB 64.8 MB/s \n","\u001b[?25hCollecting requests<3.0.0,>=2.24.0\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: httplib2<0.21.0,>=0.8 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.17.4)\n","Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.22.1)\n","Collecting google-auth-httplib2<0.2.0,>=0.1.0\n","  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n","Requirement already satisfied: google-cloud-core<3,>=0.28.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.3.2)\n","Collecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n","  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n","\u001b[K     |████████████████████████████████| 148 kB 62.0 MB/s \n","\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0\n","  Downloading google_cloud_spanner-3.24.0-py2.py3-none-any.whl (292 kB)\n","\u001b[K     |████████████████████████████████| 292 kB 54.1 MB/s \n","\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n","  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n","\u001b[K     |████████████████████████████████| 173 kB 53.3 MB/s \n","\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n","  Downloading google_cloud_dlp-3.9.2-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 60.9 MB/s \n","\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.14.1)\n","Collecting google-cloud-bigquery-storage<2.14,>=2.6.3\n","  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 60.7 MB/s \n","\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n","  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n","\u001b[K     |████████████████████████████████| 435 kB 59.5 MB/s \n","\u001b[?25hCollecting google-cloud-language<2,>=1.3.0\n","  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 2.2 MB/s \n","\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n","  Downloading google_cloud_pubsublite-1.6.0-py2.py3-none-any.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 63.4 MB/s \n","\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n","  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n","\u001b[K     |████████████████████████████████| 183 kB 69.8 MB/s \n","\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam[gcp]>=2.14.0->magenta==2.1.0) (3.3.6)\n","Collecting google-cloud-datastore<2,>=1.8.0\n","  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n","\u001b[K     |████████████████████████████████| 134 kB 49.4 MB/s \n","\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n","  Downloading google_cloud_pubsub-2.13.11-py2.py3-none-any.whl (236 kB)\n","\u001b[K     |████████████████████████████████| 236 kB 62.3 MB/s \n","\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n","  Downloading google_cloud_bigtable-1.7.3-py2.py3-none-any.whl (268 kB)\n","\u001b[K     |████████████████████████████████| 268 kB 63.1 MB/s \n","\u001b[?25hCollecting cachetools<5,>=3.1.0\n","  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n","Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.8/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.1.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (4.9)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.8.2)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.4.0)\n","Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (21.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.57.0)\n","Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.48.2)\n","Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n","  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n","Collecting overrides<7.0.0,>=6.0.1\n","  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: sqlparse>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.4.3)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4,>=1.6.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.5.0)\n","Collecting docopt\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.0.2)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.6.0)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (4.4.2)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (3.0.0)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (0.11.0)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (1.2.0)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa>=0.6.2->magenta==2.1.0) (0.4.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=1.5.3->magenta==2.1.0) (3.0.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from mir-eval>=0.4->magenta==2.1.0) (0.16.0)\n","Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n","  Downloading llvmlite-0.32.1-cp38-cp38-manylinux1_x86_64.whl (20.2 MB)\n","\u001b[K     |████████████████████████████████| 20.2 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba<0.50->magenta==2.1.0) (57.4.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (0.4.8)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa>=0.6.2->magenta==2.1.0) (1.4.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (1.24.3)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.14.0->magenta==2.1.0) (2022.9.24)\n","Collecting resampy>=0.2.2\n","  Downloading resampy-0.4.1-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 57.0 MB/s \n","\u001b[?25h  Downloading resampy-0.4.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 65.2 MB/s \n","\u001b[?25h  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 74.1 MB/s \n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.2->magenta==2.1.0) (3.1.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.10.2->librosa>=0.6.2->magenta==2.1.0) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.2->magenta==2.1.0) (2.21)\n","Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->magenta==2.1.0) (0.1.7)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->magenta==2.1.0) (1.14.1)\n","Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.8/dist-packages (from dm-sonnet->magenta==2.1.0) (0.8.10)\n","Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (2.3.3)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (22.1.0)\n","Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (1.3.5)\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Collecting note-seq\n","  Downloading note_seq-0.0.4-py3-none-any.whl (205 kB)\n","\u001b[K     |████████████████████████████████| 205 kB 54.1 MB/s \n","\u001b[?25h  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\n","\u001b[K     |████████████████████████████████| 210 kB 56.9 MB/s \n","\u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (7.9.0)\n","Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from note-seq->magenta==2.1.0) (2.1.0)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (6.0.4)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (2.11.3)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from bokeh>=0.12.0->note-seq->magenta==2.1.0) (6.0)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from intervaltree>=2.1.0->note-seq->magenta==2.1.0) (2.4.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note-seq->magenta==2.1.0) (2.0.1)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (2.0.10)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (2.6.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (0.2.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (5.1.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from IPython->note-seq->magenta==2.1.0) (4.8.0)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 60.1 MB/s \n","\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->IPython->note-seq->magenta==2.1.0) (0.8.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->note-seq->magenta==2.1.0) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->IPython->note-seq->magenta==2.1.0) (0.7.0)\n","Collecting gunicorn\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (3.1.0)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.12.11)\n","Collecting mesh-tensorflow\n","  Downloading mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n","\u001b[K     |████████████████████████████████| 385 kB 61.6 MB/s \n","\u001b[?25hCollecting gevent\n","  Downloading gevent-22.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 42.1 MB/s \n","\u001b[?25hCollecting tensorflow-gan\n","  Downloading tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n","\u001b[K     |████████████████████████████████| 367 kB 39.6 MB/s \n","\u001b[?25hRequirement already satisfied: dopamine-rl in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.0.5)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (0.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (4.64.1)\n","Collecting tensorflow-probability\n","  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 46.4 MB/s \n","\u001b[?25hCollecting tensorflow-addons\n","  Downloading tensorflow_addons-0.18.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 59.7 MB/s \n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.7.1)\n","Collecting bz2file\n","  Downloading bz2file-0.98.tar.gz (11 kB)\n","Requirement already satisfied: flask in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (1.1.4)\n","Collecting kfac\n","  Downloading kfac-0.2.4-py2.py3-none-any.whl (193 kB)\n","\u001b[K     |████████████████████████████████| 193 kB 69.8 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (4.6.0.66)\n","Collecting pypng\n","  Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from tensor2tensor->magenta==2.1.0) (0.25.2)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym->tensor2tensor->magenta==2.1.0) (4.13.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->tensor2tensor->magenta==2.1.0) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym->tensor2tensor->magenta==2.1.0) (3.10.0)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (7.1.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask->tensor2tensor->magenta==2.1.0) (1.0.1)\n","Collecting zope.interface\n","  Downloading zope.interface-5.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (261 kB)\n","\u001b[K     |████████████████████████████████| 261 kB 68.6 MB/s \n","\u001b[?25hCollecting zope.event\n","  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from gevent->tensor2tensor->magenta==2.1.0) (2.0.1)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client->tensor2tensor->magenta==2.1.0) (3.0.1)\n","Collecting kfac\n","  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n","\u001b[K     |████████████████████████████████| 191 kB 65.0 MB/s \n","\u001b[?25h  Downloading kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n","\u001b[K     |████████████████████████████████| 191 kB 67.7 MB/s \n","\u001b[?25h  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n","\u001b[K     |████████████████████████████████| 178 kB 62.1 MB/s \n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->tensor2tensor->magenta==2.1.0) (1.2.1)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (1.6.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (0.28.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.9.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (0.4.0)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.9.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.1.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (1.1.2)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (3.3.0)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (2.9.1)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (1.12)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->magenta==2.1.0) (14.0.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (0.6.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->magenta==2.1.0) (3.2.2)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tensor2tensor->magenta==2.1.0) (2.7.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (0.10.2)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (5.10.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (1.11.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (0.9.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->magenta==2.1.0) (2.3)\n","Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gan->tensor2tensor->magenta==2.1.0) (0.12.0)\n","Building wheels for collected packages: dill, google-apitools, mir-eval, pretty-midi, python-rtmidi, docopt, bz2file\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=e04fc72f3ccca214faf5a4c2acd168484d19579f65ca8415f096ccfad4874c2b\n","  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n","  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131041 sha256=2e13792931e8c73c4691ac8d4fce0d91e5334ce43d5e1c2b3e2ab666b7b2ca8c\n","  Stored in directory: /root/.cache/pip/wheels/d7/54/79/85de1824f2f4175fb4960c72afb10045d86700c3941dc73685\n","  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mir-eval: filename=mir_eval-0.7-py3-none-any.whl size=100720 sha256=b0b64a7866494ea764fe537897351484b77c2e64ce8b493e9dd141f6c4f7ccbe\n","  Stored in directory: /root/.cache/pip/wheels/20/53/83/1d50d15a666140d53eda589db005f7cb53b739c7e54711f51f\n","  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591954 sha256=d6262851316af32e42837c3bc1be01378e604d250a0b1d24e8a266340d428854\n","  Stored in directory: /root/.cache/pip/wheels/2a/5a/e3/30eeb9a99350f3f7e21258fcb132743eef1a4f49b3505e76b6\n","  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp38-cp38-linux_x86_64.whl size=417621 sha256=a309a98b29f1efb4fc2987ecb435163ce2c053be06cde423220e2fded452ef4d\n","  Stored in directory: /root/.cache/pip/wheels/23/93/e9/7d805b982c4cb5c6cec3e77e1fc6e7417a193beca2230cea52\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=f77f41595df3f99a3d5ec178b1d84de6fb5607280cfdf19abd72eb4baf8c16fb\n","  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n","  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6882 sha256=74d28d65791aef02c8bf75c5ac3b4248bb1cff3c119735f6c5fd0a42b28c4384\n","  Stored in directory: /root/.cache/pip/wheels/19/88/ce/c9430af242507ffff602cf86c5ff6a1ae5205cba5aaf21f6cc\n","Successfully built dill google-apitools mir-eval pretty-midi python-rtmidi docopt bz2file\n","Installing collected packages: cachetools, requests, llvmlite, numba, grpc-google-iam-v1, docopt, cloudpickle, zstandard, zope.interface, zope.event, tensorflow-probability, resampy, pymongo, overrides, orjson, objsize, mido, jedi, hdfs, google-cloud-pubsub, google-cloud-bigquery-storage, google-auth-httplib2, fasteners, fastavro, dill, tf-slim, tensorflow-gan, tensorflow-addons, pypng, pydub, pretty-midi, mesh-tensorflow, kfac, gunicorn, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsublite, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-apitools, gevent, bz2file, apache-beam, tensor2tensor, sox, sk-video, python-rtmidi, pygtrie, note-seq, mir-eval, dm-sonnet, magenta\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.2.0\n","    Uninstalling cachetools-5.2.0:\n","      Successfully uninstalled cachetools-5.2.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: llvmlite\n","    Found existing installation: llvmlite 0.39.1\n","    Uninstalling llvmlite-0.39.1:\n","      Successfully uninstalled llvmlite-0.39.1\n","  Attempting uninstall: numba\n","    Found existing installation: numba 0.56.4\n","    Uninstalling numba-0.56.4:\n","      Successfully uninstalled numba-0.56.4\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.5.0\n","    Uninstalling cloudpickle-1.5.0:\n","      Successfully uninstalled cloudpickle-1.5.0\n","  Attempting uninstall: tensorflow-probability\n","    Found existing installation: tensorflow-probability 0.17.0\n","    Uninstalling tensorflow-probability-0.17.0:\n","      Successfully uninstalled tensorflow-probability-0.17.0\n","  Attempting uninstall: resampy\n","    Found existing installation: resampy 0.4.2\n","    Uninstalling resampy-0.4.2:\n","      Successfully uninstalled resampy-0.4.2\n","  Attempting uninstall: pymongo\n","    Found existing installation: pymongo 4.3.3\n","    Uninstalling pymongo-4.3.3:\n","      Successfully uninstalled pymongo-4.3.3\n","  Attempting uninstall: google-cloud-bigquery-storage\n","    Found existing installation: google-cloud-bigquery-storage 2.16.2\n","    Uninstalling google-cloud-bigquery-storage-2.16.2:\n","      Successfully uninstalled google-cloud-bigquery-storage-2.16.2\n","  Attempting uninstall: google-auth-httplib2\n","    Found existing installation: google-auth-httplib2 0.0.4\n","    Uninstalling google-auth-httplib2-0.0.4:\n","      Successfully uninstalled google-auth-httplib2-0.0.4\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.6\n","    Uninstalling dill-0.3.6:\n","      Successfully uninstalled dill-0.3.6\n","  Attempting uninstall: google-cloud-language\n","    Found existing installation: google-cloud-language 2.6.1\n","    Uninstalling google-cloud-language-2.6.1:\n","      Successfully uninstalled google-cloud-language-2.6.1\n","  Attempting uninstall: google-cloud-datastore\n","    Found existing installation: google-cloud-datastore 2.9.0\n","    Uninstalling google-cloud-datastore-2.9.0:\n","      Successfully uninstalled google-cloud-datastore-2.9.0\n","Successfully installed apache-beam-2.43.0 bz2file-0.98 cachetools-4.2.4 cloudpickle-2.2.0 dill-0.3.1.1 dm-sonnet-2.0.0 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 gevent-22.10.2 google-apitools-0.5.31 google-auth-httplib2-0.1.0 google-cloud-bigquery-storage-2.13.2 google-cloud-bigtable-1.7.3 google-cloud-datastore-1.15.5 google-cloud-dlp-3.9.2 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.11 google-cloud-pubsublite-1.6.0 google-cloud-recommendations-ai-0.7.1 google-cloud-spanner-3.24.0 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 grpc-google-iam-v1-0.12.4 gunicorn-20.1.0 hdfs-2.7.0 jedi-0.18.2 kfac-0.2.0 llvmlite-0.32.1 magenta-2.1.0 mesh-tensorflow-0.1.21 mido-1.2.6 mir-eval-0.7 note-seq-0.0.3 numba-0.49.1 objsize-0.5.2 orjson-3.8.3 overrides-6.5.0 pretty-midi-0.2.9 pydub-0.25.1 pygtrie-2.5.0 pymongo-3.13.0 pypng-0.20220715.0 python-rtmidi-1.1.2 requests-2.28.1 resampy-0.3.1 sk-video-1.1.10 sox-1.4.1 tensor2tensor-1.15.7 tensorflow-addons-0.18.0 tensorflow-gan-2.1.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.5.2 zstandard-0.19.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]}}},"metadata":{}}],"source":["!pip install magenta==2.1.0"]},{"cell_type":"code","source":["import numpy as np\n","import pathlib\n","import zipfile\n","import os\n","import pandas as pd\n","import IPython\n","import collections\n","import note_seq\n","import hashlib\n","\n","from note_seq import abc_parser\n","from note_seq import midi_io\n","from note_seq import musicxml_reader\n","\n","from magenta.common import merge_hparams\n","from magenta.contrib import training as contrib_training\n","from magenta.models.music_vae import MusicVAE\n","from magenta.models.music_vae import lstm_models\n","from magenta.models.music_vae import data\n","from magenta.scripts import convert_dir_to_note_sequences\n","from magenta.scripts.convert_dir_to_note_sequences import convert_directory\n","from magenta.models.music_vae import configs\n","from magenta.models.music_vae.trained_model import TrainedModel\n","import tensorflow.compat.v1 as tf\n","import tf_slim"],"metadata":{"id":"K64aVkLnfbHK","executionInfo":{"status":"ok","timestamp":1670373698552,"user_tz":-540,"elapsed":20812,"user":{"displayName":"강가람","userId":"05000797890779455895"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e728725d-c244-4e66-d41d-3e526aec6f67"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/resampy/interpn.py:114: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n","  _resample_loop_p(x, t_out, interp_win, interp_delta, num_table, scale, y)\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Colab Notebooks/MusicVAE'\n","os.mkdir(path + '/data_dir')"],"metadata":{"id":"fp4GdiXZJa4B","executionInfo":{"status":"ok","timestamp":1670373703242,"user_tz":-540,"elapsed":616,"user":{"displayName":"강가람","userId":"05000797890779455895"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load the full GMD with MIDI only (no audio) as a tf.data.Dataset\n","# dataset = tfds.load(name='groove/4bar-midionly', split=tfds.Split.TRAIN, try_gcs=True)"],"metadata":{"id":"vXT2aaMGFtNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\"\n","dir = tf.keras.utils.get_file(origin=url, fname= path + '/data_dir/data.zip', extract=True)\n","data_dir = pathlib.Path(dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwceackK8RPB","executionInfo":{"status":"ok","timestamp":1670373742027,"user_tz":-540,"elapsed":1003,"user":{"displayName":"강가람","userId":"05000797890779455895"}},"outputId":"4cf56488-ff6a-49b4-9471-1d07532aa42d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\n","3260318/3260318 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["zipfile.ZipFile(path + '/data_dir/data.zip').extractall(path)"],"metadata":{"id":"HB2k6hw_dnum","executionInfo":{"status":"ok","timestamp":1670373887677,"user_tz":-540,"elapsed":8386,"user":{"displayName":"강가람","userId":"05000797890779455895"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["data_root= path + '/groove'\n","tfrec_root = path + '/data_dir/music.tfrecord'"],"metadata":{"id":"9r27QpGm0Q1O","executionInfo":{"status":"ok","timestamp":1670374010762,"user_tz":-540,"elapsed":334,"user":{"displayName":"강가람","userId":"05000797890779455895"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["##### ?converted midi file check...\n","convert_directory(data_root,tfrec_root,recursive=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54W9Ij0o0QyY","executionInfo":{"status":"ok","timestamp":1670374044536,"user_tz":-540,"elapsed":31237,"user":{"displayName":"강가람","userId":"05000797890779455895"}},"outputId":"cfcacc3c-ada3-49b5-aa0f-758b14909e99"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/LICENSE\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/README\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/info.csv\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer8/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer8/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer8/eval_session/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer8/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer6/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer6/session3/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer6/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer6/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer1/session3/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer1/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer1/eval_session/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer1/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer7/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer7/session3/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer7/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer7/eval_session/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer7/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer10/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer10/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer9/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer9/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer2/session3/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer2/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer2/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer5/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer5/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer5/eval_session/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer5/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer4/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer4/session1/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer3/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer3/session2/Icon\n","WARNING:tensorflow:Unable to find a converter for file /content/drive/MyDrive/Colab Notebooks/MusicVAE/groove/drummer3/session1/Icon\n"]}]},{"cell_type":"code","source":["# configs.py\n","\n","class Config(collections.namedtuple(\n","    'Config',\n","    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n","     'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n","\n","    def values(self):\n","        return self._asdict()\n","\n","Config.__new__.__defaults__ = (None,) * len(Config._fields)\n","\n","def update_config(config, update_dict):\n","    config_dict = config.values()\n","    config_dict.update(update_dict)\n","    return Config(**config_dict)\n","\n","CONFIG_MAP = {}\n","\n","HParams = contrib_training.HParams\n","\n","CONFIG_MAP['groovae_4bar'] = Config(\n","    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n","                   lstm_models.GrooveLstmDecoder()),\n","    hparams=merge_hparams(\n","        lstm_models.get_default_hparams(),\n","        HParams(\n","            batch_size=512,\n","            max_seq_len=16 * 4,\n","            z_size=256,\n","            enc_rnn_size=[512],\n","            dec_rnn_size=[256, 256],\n","            max_beta=0.2,\n","            free_bits=48,\n","            dropout_keep_prob=0.3,\n","        )),\n","    note_sequence_augmenter=None,\n","    data_converter=data.GrooveConverter(\n","        split_bars=4, steps_per_quarter=4, quarters_per_bar=4,\n","        max_tensors_per_notesequence=20,\n","        pitch_classes=data.ROLAND_DRUM_PITCH_CLASSES,\n","\n","        inference_pitch_classes=data.REDUCED_DRUM_PITCH_CLASSES),\n","    # tfds_name='groove/4bar-midionly',\n","    train_examples_path= '/content/data_dir/music.tfrecord')"],"metadata":{"id":"fvWur-000Qr3","executionInfo":{"status":"ok","timestamp":1670374233395,"user_tz":-540,"elapsed":351,"user":{"displayName":"강가람","userId":"05000797890779455895"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# music_vae_train.py\n","\n","def _trial_summary(hparams, examples_path, output_dir):\n","  \"\"\"Writes a tensorboard text summary of the trial.\"\"\"\n","\n","  examples_path_summary = tf.summary.text(\n","      'examples_path', tf.constant(examples_path, name='examples_path'),\n","      collections=[])\n","\n","  hparams_dict = hparams.values()\n","\n","  # Create a markdown table from hparams.\n","  header = '| Key | Value |\\n| :--- | :--- |\\n'\n","  keys = sorted(hparams_dict.keys())\n","  lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n","  hparams_table = header + '\\n'.join(lines) + '\\n'\n","\n","  hparam_summary = tf.summary.text(\n","      'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n","\n","  with tf.Session() as sess:\n","    writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n","    writer.add_summary(examples_path_summary.eval())\n","    writer.add_summary(hparam_summary.eval())\n","    writer.close()\n","\n","\n","def _get_input_tensors(dataset, config):\n","  \"\"\"Get input tensors from dataset.\"\"\"\n","  batch_size = config.hparams.batch_size\n","  iterator = tf.data.make_one_shot_iterator(dataset)\n","  (input_sequence, output_sequence, control_sequence,\n","   sequence_length) = iterator.get_next()\n","  input_sequence.set_shape(\n","      [batch_size, None, config.data_converter.input_depth])\n","  output_sequence.set_shape(\n","      [batch_size, None, config.data_converter.output_depth])\n","  \n","  if not config.data_converter.control_depth:\n","    control_sequence = None\n","\n","  else:\n","    control_sequence.set_shape(\n","        [batch_size, None, config.data_converter.control_depth])\n","    sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n","\n","  return {\n","      'input_sequence': input_sequence,\n","      'output_sequence': output_sequence,\n","      'control_sequence': control_sequence,\n","      'sequence_length': sequence_length\n","  }\n","\n","def train(train_dir,\n","          config,\n","          dataset_fn,\n","          checkpoints_to_keep=5,\n","          keep_checkpoint_every_n_hours=1,\n","          num_steps=None,\n","          master='',\n","          num_sync_workers=0,\n","          num_ps_tasks=0,\n","          task=0):\n","    \n","    # global train_op\n","\n","#==== train loop ====\n","    tf.gfile.MakeDirs(train_dir)\n","    is_chief = (task == 0)\n","    \n","    with tf.Graph().as_default():\n","        with tf.device(tf.train.replica_device_setter(\n","            num_ps_tasks, merge_devices=True)):\n","            \n","            model = config.model\n","            model.build(config.hparams,\n","                        config.data_converter.output_depth,\n","                        is_training=True)\n","            optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n","\n","            hooks = []\n","            if num_sync_workers:\n","                optimizer = tf.train.SyncReplicasOptimizer(\n","                    optimizer,num_sync_workers)\n","                hooks.append(optimizer.make_session_run_hook(is_chief))\n","\n","            grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n","            global_norm = tf.global_norm(grads)\n","            tf.summary.scalar('global_norm', global_norm)\n","            \n","            if config.hparams.clip_mode == 'value':\n","                g = config.hparams.grad_clip\n","                clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n","            elif config.hparams.clip_mode == 'global_norm':\n","                clipped_grads = tf.cond(\n","                    global_norm < config.hparams.grad_norm_clip_to_zero,\n","                    lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n","                        grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n","                    lambda: [tf.zeros(tf.shape(g)) for g in grads])\n","            else:\n","                raise ValueError(\n","                    'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n","                train_op = optimizer.apply_gradients(\n","                    list(zip(clipped_grads, var_list)),\n","                    global_step=model.global_step,\n","                    name='train_step')\n","            logging_dict = {'global_step': model.global_step,\n","                            'loss': model.loss}\n","            \n","            hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n","            if num_steps:\n","                hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n","                \n","            scaffold = tf.train.Scaffold(\n","                saver=tf.train.Saver(\n","                    max_to_keep=checkpoints_to_keep,\n","                    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n","            \n","            tf_slim.training.train(\n","                train_op=train_op,\n","                logdir=train_dir,\n","                scaffold=scaffold,\n","                hooks=hooks,\n","                save_checkpoint_secs=60,\n","                master=master,\n","                is_chief=is_chief)\n","\n","def run(config_map,\n","        tf_file_reader=tf.data.TFRecordDataset,\n","        file_reader=tf.python_io.tf_record_iterator,\n","        is_training=True):\n","    config = config_map['groovae_4bar']\n","    train_dir = '/content/train'\n","    num_steps = 5000\n","    \n","    def dataset_fn():\n","        return data.get_dataset(\n","            config,\n","            tf_file_reader=tf_file_reader,\n","            is_training=True,\n","            cache_dataset=True)\n","    \n","    if is_training == True:\n","        train(\n","            train_dir,\n","            config=config,\n","            dataset_fn=dataset_fn,\n","            num_steps=num_steps)      \n","    \n","    else:\n","        print(\"EVAL\")"],"metadata":{"id":"dZXw6Npz5lAx","executionInfo":{"status":"ok","timestamp":1670374335756,"user_tz":-540,"elapsed":413,"user":{"displayName":"강가람","userId":"05000797890779455895"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["##### ?error checking...\n","run(CONFIG_MAP)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":392},"id":"_4HZaijZ5k_7","executionInfo":{"status":"error","timestamp":1670374340308,"user_tz":-540,"elapsed":529,"user":{"displayName":"강가람","userId":"05000797890779455895"}},"outputId":"6dd26704-edb3-46d2-ee08-948aeb17da46"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-f2f54e317140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG_MAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-2c8d06ca9d2a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config_map, tf_file_reader, file_reader, is_training)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         train(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-2c8d06ca9d2a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dir, config, dataset_fn, checkpoints_to_keep, keep_checkpoint_every_n_hours, num_steps, master, num_sync_workers, num_ps_tasks, task)\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_converter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         is_training=True)\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_get_input_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mhooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-2c8d06ca9d2a>\u001b[0m in \u001b[0;36mdataset_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataset_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         return data.get_dataset(\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mtf_file_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_file_reader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/magenta/models/music_vae/data.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(config, tf_file_reader, is_training, cache_dataset)\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0mnum_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   1795\u001b[0m           'No files were found matching examples path: %s' %  examples_path)\n\u001b[1;32m   1796\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No files were found matching examples path: /content/data_dir/music.tfrecord"]}]},{"cell_type":"code","source":["from magenta.models.music_vae import music_vae_train"],"metadata":{"id":"wAOD2uBmp3nl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drum_config = configs.CONFIG_MAP['groovae_4bar']\n","model = TrainedModel(drum_config, batch_size=2, checkpoint_dir_or_path= path +'/train')\n","\n","generated_sequence = model.sample(n=1, length=64, temperature=0.5)\n","note_seq.sequence_proto_to_midi_file(generated_sequence[0], path + '/gen_midi/drum_4bar_new.mid')"],"metadata":{"id":"DQ1wacq95k_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ei6PBAWD5kzE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TGAm2fkBHkOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# \"\"\"MusicVAE training script.\"\"\"\n","# import os\n","\n","# from magenta.models.music_vae import configs\n","# from magenta.models.music_vae import data\n","# import tensorflow.compat.v1 as tf\n","# import tf_slim\n","\n","# flags = tf.app.flags\n","# FLAGS = flags.FLAGS\n","\n","# flags.DEFINE_string(\n","#     'master', '',\n","#     'The TensorFlow master to use.')\n","# flags.DEFINE_string(\n","#     'examples_path', None,\n","#     'Path to a TFRecord file of NoteSequence examples. Overrides the config.')\n","# flags.DEFINE_string(\n","#     'tfds_name', None,\n","#     'TensorFlow Datasets dataset name to use. Overrides the config.')\n","# flags.DEFINE_string(\n","#     'run_dir', None,\n","#     'Path where checkpoints and summary events will be located during '\n","#     'training and evaluation. Separate subdirectories `train` and `eval` '\n","#     'will be created within this directory.')\n","# flags.DEFINE_integer(\n","#     'num_steps', 200000,\n","#     'Number of training steps or `None` for infinite.')\n","# flags.DEFINE_integer(\n","#     'eval_num_batches', None,\n","#     'Number of batches to use during evaluation or `None` for all batches '\n","#     'in the data source.')\n","# flags.DEFINE_integer(\n","#     'checkpoints_to_keep', 100,\n","#     'Maximum number of checkpoints to keep in `train` mode or 0 for infinite.')\n","# flags.DEFINE_integer(\n","#     'keep_checkpoint_every_n_hours', 1,\n","#     'In addition to checkpoints_to_keep, keep a checkpoint every N hours.')\n","# flags.DEFINE_string(\n","#     'mode', 'train',\n","#     'Which mode to use (`train` or `eval`).')\n","# flags.DEFINE_string(\n","#     'config', '',\n","#     'The name of the config to use.')\n","# flags.DEFINE_string(\n","#     'hparams', '',\n","#     'A comma-separated list of `name=value` hyperparameter values to merge '\n","#     'with those in the config.')\n","# flags.DEFINE_bool(\n","#     'cache_dataset', True,\n","#     'Whether to cache the dataset in memory for improved training speed. May '\n","#     'cause memory errors for very large datasets.')\n","# flags.DEFINE_integer(\n","#     'task', 0,\n","#     'The task number for this worker.')\n","# flags.DEFINE_integer(\n","#     'num_ps_tasks', 0,\n","#     'The number of parameter server tasks.')\n","# flags.DEFINE_integer(\n","#     'num_sync_workers', 0,\n","#     'The number of synchronized workers.')\n","# flags.DEFINE_string(\n","#     'eval_dir_suffix', '',\n","#     'Suffix to add to eval output directory.')\n","# flags.DEFINE_string(\n","#     'log', 'INFO',\n","#     'The threshold for what messages will be logged: '\n","#     'DEBUG, INFO, WARN, ERROR, or FATAL.')\n","\n","\n","# # Should not be called from within the graph to avoid redundant summaries.\n","# def _trial_summary(hparams, examples_path, output_dir):\n","#   \"\"\"Writes a tensorboard text summary of the trial.\"\"\"\n","\n","#   examples_path_summary = tf.summary.text(\n","#       'examples_path', tf.constant(examples_path, name='examples_path'),\n","#       collections=[])\n","\n","#   hparams_dict = hparams.values()\n","\n","#   # Create a markdown table from hparams.\n","#   header = '| Key | Value |\\n| :--- | :--- |\\n'\n","#   keys = sorted(hparams_dict.keys())\n","#   lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n","#   hparams_table = header + '\\n'.join(lines) + '\\n'\n","\n","#   hparam_summary = tf.summary.text(\n","#       'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n","\n","#   with tf.Session() as sess:\n","#     writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n","#     writer.add_summary(examples_path_summary.eval())\n","#     writer.add_summary(hparam_summary.eval())\n","#     writer.close()\n","\n","\n","# def _get_input_tensors(dataset, config):\n","#   \"\"\"Get input tensors from dataset.\"\"\"\n","#   batch_size = config.hparams.batch_size\n","#   iterator = tf.data.make_one_shot_iterator(dataset)\n","#   (input_sequence, output_sequence, control_sequence,\n","#    sequence_length) = iterator.get_next()\n","#   input_sequence.set_shape(\n","#       [batch_size, None, config.data_converter.input_depth])\n","#   output_sequence.set_shape(\n","#       [batch_size, None, config.data_converter.output_depth])\n","#   if not config.data_converter.control_depth:\n","#     control_sequence = None\n","#   else:\n","#     control_sequence.set_shape(\n","#         [batch_size, None, config.data_converter.control_depth])\n","#   sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n","\n","#   return {\n","#       'input_sequence': input_sequence,\n","#       'output_sequence': output_sequence,\n","#       'control_sequence': control_sequence,\n","#       'sequence_length': sequence_length\n","#   }\n","\n","\n","# def train(train_dir,\n","#           config,\n","#           dataset_fn,\n","#           checkpoints_to_keep=5,\n","#           keep_checkpoint_every_n_hours=1,\n","#           num_steps=None,\n","#           master='',\n","#           num_sync_workers=0,\n","#           num_ps_tasks=0,\n","#           task=0):\n","#   \"\"\"Train loop.\"\"\"\n","#   tf.gfile.MakeDirs(train_dir)\n","#   is_chief = (task == 0)\n","#   if is_chief:\n","#     _trial_summary(\n","#         config.hparams, config.train_examples_path or config.tfds_name,\n","#         train_dir)\n","#   with tf.Graph().as_default():\n","#     with tf.device(tf.train.replica_device_setter(\n","#         num_ps_tasks, merge_devices=True)):\n","\n","#       model = config.model\n","#       model.build(config.hparams,\n","#                   config.data_converter.output_depth,\n","#                   is_training=True)\n","\n","#       optimizer = model.train(**_get_input_tensors(dataset_fn(), config))\n","\n","#       hooks = []\n","#       if num_sync_workers:\n","#         optimizer = tf.train.SyncReplicasOptimizer(\n","#             optimizer,\n","#             num_sync_workers)\n","#         hooks.append(optimizer.make_session_run_hook(is_chief))\n","\n","#       grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n","#       global_norm = tf.global_norm(grads)\n","#       tf.summary.scalar('global_norm', global_norm)\n","\n","#       if config.hparams.clip_mode == 'value':\n","#         g = config.hparams.grad_clip\n","#         clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n","#       elif config.hparams.clip_mode == 'global_norm':\n","#         clipped_grads = tf.cond(\n","#             global_norm < config.hparams.grad_norm_clip_to_zero,\n","#             lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n","#                 grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n","#             lambda: [tf.zeros(tf.shape(g)) for g in grads])\n","#       else:\n","#         raise ValueError(\n","#             'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n","#       train_op = optimizer.apply_gradients(\n","#           list(zip(clipped_grads, var_list)),\n","#           global_step=model.global_step,\n","#           name='train_step')\n","\n","#       logging_dict = {'global_step': model.global_step,\n","#                       'loss': model.loss}\n","\n","#       hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n","#       if num_steps:\n","#         hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n","\n","#       scaffold = tf.train.Scaffold(\n","#           saver=tf.train.Saver(\n","#               max_to_keep=checkpoints_to_keep,\n","#               keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n","#       tf_slim.training.train(\n","#           train_op=train_op,\n","#           logdir=train_dir,\n","#           scaffold=scaffold,\n","#           hooks=hooks,\n","#           save_checkpoint_secs=60,\n","#           master=master,\n","#           is_chief=is_chief)\n","\n","\n","# def evaluate(train_dir,\n","#              eval_dir,\n","#              config,\n","#              dataset_fn,\n","#              num_batches,\n","#              master=''):\n","#   \"\"\"Evaluate the model repeatedly.\"\"\"\n","#   tf.gfile.MakeDirs(eval_dir)\n","\n","#   _trial_summary(\n","#       config.hparams, config.eval_examples_path or config.tfds_name, eval_dir)\n","#   with tf.Graph().as_default():\n","#     model = config.model\n","#     model.build(config.hparams,\n","#                 config.data_converter.output_depth,\n","#                 is_training=False)\n","\n","#     eval_op = model.eval(\n","#         **_get_input_tensors(dataset_fn().take(num_batches), config))\n","\n","#     hooks = [\n","#         tf_slim.evaluation.StopAfterNEvalsHook(num_batches),\n","#         tf_slim.evaluation.SummaryAtEndHook(eval_dir)\n","#     ]\n","#     tf_slim.evaluation.evaluate_repeatedly(\n","#         train_dir,\n","#         eval_ops=eval_op,\n","#         hooks=hooks,\n","#         eval_interval_secs=60,\n","#         master=master)\n","\n","\n","# def run(config_map,\n","#         tf_file_reader=tf.data.TFRecordDataset,\n","#         file_reader=tf.python_io.tf_record_iterator):\n","#   \"\"\"Load model params, save config file and start trainer.\n","#   Args:\n","#     config_map: Dictionary mapping configuration name to Config object.\n","#     tf_file_reader: The tf.data.Dataset class to use for reading files.\n","#     file_reader: The Python reader to use for reading files.\n","#   Raises:\n","#     ValueError: if required flags are missing or invalid.\n","#   \"\"\"\n","#   if not FLAGS.run_dir:\n","#     raise ValueError('Invalid run directory: %s' % FLAGS.run_dir)\n","#   run_dir = os.path.expanduser(FLAGS.run_dir)\n","#   train_dir = os.path.join(run_dir, 'train')\n","\n","#   if FLAGS.mode not in ['train', 'eval']:\n","#     raise ValueError('Invalid mode: %s' % FLAGS.mode)\n","\n","#   if FLAGS.config not in config_map:\n","#     raise ValueError('Invalid config: %s' % FLAGS.config)\n","#   config = config_map[FLAGS.config]\n","#   if FLAGS.hparams:\n","#     config.hparams.parse(FLAGS.hparams)\n","#   config_update_map = {}\n","#   if FLAGS.examples_path:\n","#     config_update_map['%s_examples_path' % FLAGS.mode] = os.path.expanduser(\n","#         FLAGS.examples_path)\n","#   if FLAGS.tfds_name:\n","#     if FLAGS.examples_path:\n","#       raise ValueError(\n","#           'At most one of --examples_path and --tfds_name can be set.')\n","#     config_update_map['tfds_name'] = FLAGS.tfds_name\n","#     config_update_map['eval_examples_path'] = None\n","#     config_update_map['train_examples_path'] = None\n","#   config = configs.update_config(config, config_update_map)\n","#   if FLAGS.num_sync_workers:\n","#     config.hparams.batch_size //= FLAGS.num_sync_workers\n","\n","#   if FLAGS.mode == 'train':\n","#     is_training = True\n","#   elif FLAGS.mode == 'eval':\n","#     is_training = False\n","#   else:\n","#     raise ValueError('Invalid mode: {}'.format(FLAGS.mode))\n","\n","#   def dataset_fn():\n","#     return data.get_dataset(\n","#         config,\n","#         tf_file_reader=tf_file_reader,\n","#         is_training=is_training,\n","#         cache_dataset=FLAGS.cache_dataset)\n","\n","#   if is_training:\n","#     train(\n","#         train_dir,\n","#         config=config,\n","#         dataset_fn=dataset_fn,\n","#         checkpoints_to_keep=FLAGS.checkpoints_to_keep,\n","#         keep_checkpoint_every_n_hours=FLAGS.keep_checkpoint_every_n_hours,\n","#         num_steps=FLAGS.num_steps,\n","#         master=FLAGS.master,\n","#         num_sync_workers=FLAGS.num_sync_workers,\n","#         num_ps_tasks=FLAGS.num_ps_tasks,\n","#         task=FLAGS.task)\n","#   else:\n","#     num_batches = FLAGS.eval_num_batches or data.count_examples(\n","#         config.eval_examples_path,\n","#         config.tfds_name,\n","#         config.data_converter,\n","#         file_reader) // config.hparams.batch_size\n","#     eval_dir = os.path.join(run_dir, 'eval' + FLAGS.eval_dir_suffix)\n","#     evaluate(\n","#         train_dir,\n","#         eval_dir,\n","#         config=config,\n","#         dataset_fn=dataset_fn,\n","#         num_batches=num_batches,\n","#         master=FLAGS.master)\n","\n","\n","# def main(unused_argv):\n","#   tf.logging.set_verbosity(FLAGS.log)\n","#   run(configs.CONFIG_MAP)\n","\n","\n","# def console_entry_point():\n","#   tf.disable_v2_behavior()\n","#   tf.app.run(main)\n","\n","\n","# if __name__ == '__main__':\n","#   console_entry_point()"],"metadata":{"id":"AFZLsa6mHkFM"},"execution_count":null,"outputs":[]}]}